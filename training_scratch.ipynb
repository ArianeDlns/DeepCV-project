{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#! pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:27.159647Z","iopub.execute_input":"2022-04-07T08:31:27.160274Z","iopub.status.idle":"2022-04-07T08:31:27.164793Z","shell.execute_reply.started":"2022-04-07T08:31:27.160234Z","shell.execute_reply":"2022-04-07T08:31:27.163993Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# For plotting\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# For conversion\nfrom skimage.color import lab2rgb, rgb2lab, rgb2gray\nfrom skimage import io\n# For everything\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# For our model\nimport torchvision.models as models\nfrom torchvision import datasets, transforms\n# For utilities\nimport os, shutil, time\n#from utils import *","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:27.170948Z","iopub.execute_input":"2022-04-07T08:31:27.171176Z","iopub.status.idle":"2022-04-07T08:31:27.179428Z","shell.execute_reply.started":"2022-04-07T08:31:27.171151Z","shell.execute_reply":"2022-04-07T08:31:27.178405Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Move data into training and validation directories\nimport os\nfrom tqdm import tqdm\nimport glob\n\nINPUT_PATH = '../input/pokemonclassification/PokemonData/'\n\nos.makedirs('images/train/class/', exist_ok=True) # 6,000 images\nos.makedirs('images/val/class/', exist_ok=True)   #  200 images\n\nfor i, file in tqdm(enumerate(glob.glob(\"../input/pokemonclassification/PokemonData/*/*.jpg\"))):\n  if i < 200: # first 200 will be val\n    shutil.copy(file, 'images/val/class/')\n  else: # others will be val\n    shutil.copy(file, 'images/train/class/')","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:27.181585Z","iopub.execute_input":"2022-04-07T08:31:27.181874Z","iopub.status.idle":"2022-04-07T08:31:34.560390Z","shell.execute_reply.started":"2022-04-07T08:31:27.181841Z","shell.execute_reply":"2022-04-07T08:31:34.559641Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Make sure the images are there\nfrom IPython.display import Image, display\ndisplay(Image(filename='images/train/class/2b7870f2174e4a1d9e13bc0bb71fd35b.jpg'))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.561895Z","iopub.execute_input":"2022-04-07T08:31:34.562285Z","iopub.status.idle":"2022-04-07T08:31:34.577778Z","shell.execute_reply.started":"2022-04-07T08:31:34.562238Z","shell.execute_reply":"2022-04-07T08:31:34.576171Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ColorizationNet(nn.Module):\n  def __init__(self, input_size=128):\n    super(ColorizationNet, self).__init__()\n    MIDLEVEL_FEATURE_SIZE = 128\n\n    ## First half: ResNet\n    resnet = models.resnet18(num_classes=365) \n    # Change first conv layer to accept single-channel (grayscale) input\n    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n    # Extract midlevel features from ResNet-gray\n    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n\n    ## Second half: Upsampling\n    self.upsample = nn.Sequential(     \n      nn.Conv2d(MIDLEVEL_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(128),\n      nn.ReLU(),\n      nn.Upsample(scale_factor=2),\n      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(64),\n      nn.ReLU(),\n      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(64),\n      nn.ReLU(),\n      nn.Upsample(scale_factor=2),\n      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n      nn.BatchNorm2d(32),\n      nn.ReLU(),\n      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n      nn.Upsample(scale_factor=2)\n    )\n\n  def forward(self, input):\n\n    # Pass input through ResNet-gray to extract features\n    midlevel_features = self.midlevel_resnet(input)\n\n    # Upsample to get colors\n    output = self.upsample(midlevel_features)\n    return output","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.578642Z","iopub.execute_input":"2022-04-07T08:31:34.579458Z","iopub.status.idle":"2022-04-07T08:31:34.591283Z","shell.execute_reply.started":"2022-04-07T08:31:34.579423Z","shell.execute_reply":"2022-04-07T08:31:34.590382Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = ColorizationNet()\n# Check if GPU is available\nuse_gpu = torch.cuda.is_available()\nprint(use_gpu)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.593720Z","iopub.execute_input":"2022-04-07T08:31:34.594435Z","iopub.status.idle":"2022-04-07T08:31:34.871906Z","shell.execute_reply.started":"2022-04-07T08:31:34.594399Z","shell.execute_reply":"2022-04-07T08:31:34.871016Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.873421Z","iopub.execute_input":"2022-04-07T08:31:34.873677Z","iopub.status.idle":"2022-04-07T08:31:34.879402Z","shell.execute_reply.started":"2022-04-07T08:31:34.873641Z","shell.execute_reply":"2022-04-07T08:31:34.877431Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.880933Z","iopub.execute_input":"2022-04-07T08:31:34.882000Z","iopub.status.idle":"2022-04-07T08:31:34.888229Z","shell.execute_reply.started":"2022-04-07T08:31:34.881957Z","shell.execute_reply":"2022-04-07T08:31:34.887376Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class GrayscaleImageFolder(datasets.ImageFolder):\n  '''Custom images folder, which converts images to grayscale before loading'''\n  def __getitem__(self, index):\n    path, target = self.imgs[index]\n    img = self.loader(path)\n    if self.transform is not None:\n      img_original = self.transform(img)\n      img_original = np.asarray(img_original)\n      img_lab = rgb2lab(img_original)\n      img_lab = (img_lab + 128) / 255\n      img_ab = img_lab[:, :, 1:3]\n      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n      img_original = rgb2gray(img_original)\n      img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n    if self.target_transform is not None:\n      target = self.target_transform(target)\n    return img_original, img_ab, target","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.889542Z","iopub.execute_input":"2022-04-07T08:31:34.890187Z","iopub.status.idle":"2022-04-07T08:31:34.901057Z","shell.execute_reply.started":"2022-04-07T08:31:34.890148Z","shell.execute_reply":"2022-04-07T08:31:34.900192Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Training\ntrain_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\ntrain_imagefolder = GrayscaleImageFolder('images/train', train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64, shuffle=True)\n\n# Validation \nval_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\nval_imagefolder = GrayscaleImageFolder('images/val' , val_transforms)\nval_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.902188Z","iopub.execute_input":"2022-04-07T08:31:34.902810Z","iopub.status.idle":"2022-04-07T08:31:34.952788Z","shell.execute_reply.started":"2022-04-07T08:31:34.902757Z","shell.execute_reply":"2022-04-07T08:31:34.951977Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, model, criterion, save_images, epoch):\n  model.eval()\n\n  # Prepare value counters and timers\n  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n\n  end = time.time()\n  already_saved_images = False\n  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n    data_time.update(time.time() - end)\n\n    # Use GPU\n    if use_gpu: \n      input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n\n    # Run model and record loss\n    output_ab = model(input_gray) # throw away class predictions\n    loss = criterion(output_ab, input_ab)\n    losses.update(loss.item(), input_gray.size(0))\n\n    # Save images to file\n    if save_images and not already_saved_images:\n      already_saved_images = True\n      for j in range(min(len(output_ab), 10)): # save at most 5 images\n        save_path = {'grayscale': 'outputs/gray/', 'colorized': 'outputs/color/'}\n        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n\n    # Record time to do forward passes and save images\n    batch_time.update(time.time() - end)\n    end = time.time()\n\n    # Print model accuracy -- in the code below, val refers to both value and validation\n    if i % 25 == 0:\n      print('Validate: [{0}/{1}]\\t'\n            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n             i, len(val_loader), batch_time=batch_time, loss=losses))\n\n  print('Finished validation.')\n  return losses.avg","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.953834Z","iopub.execute_input":"2022-04-07T08:31:34.954071Z","iopub.status.idle":"2022-04-07T08:31:34.965813Z","shell.execute_reply.started":"2022-04-07T08:31:34.954039Z","shell.execute_reply":"2022-04-07T08:31:34.965000Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, model, criterion, optimizer, epoch):\n  print('Starting training epoch {}'.format(epoch))\n  model.train()\n  \n  # Prepare value counters and timers\n  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n\n  end = time.time()\n  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n    \n    # Use GPU if available\n    if use_gpu: \n      input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n\n    # Record time to load data (above)\n    data_time.update(time.time() - end)\n\n    # Run forward pass\n    output_ab = model(input_gray) \n    loss = criterion(output_ab, input_ab) \n    losses.update(loss.item(), input_gray.size(0))\n\n    # Compute gradient and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Record time to do forward and backward passes\n    batch_time.update(time.time() - end)\n    end = time.time()\n\n    # Print model accuracy -- in the code below, val refers to value, not validation\n    if i % 25 == 0:\n      print('Epoch: [{0}][{1}/{2}]\\t'\n            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n              epoch, i, len(train_loader), batch_time=batch_time,\n             data_time=data_time, loss=losses)) \n\n  print('Finished training epoch {}'.format(epoch))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.968668Z","iopub.execute_input":"2022-04-07T08:31:34.969047Z","iopub.status.idle":"2022-04-07T08:31:34.981201Z","shell.execute_reply.started":"2022-04-07T08:31:34.969011Z","shell.execute_reply":"2022-04-07T08:31:34.980349Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n  '''A handy class from the PyTorch ImageNet tutorial''' \n  def __init__(self):\n    self.reset()\n  def reset(self):\n    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n\ndef to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n  '''Show/save rgb image from grayscale and ab channels\n     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n  plt.clf() # clear matplotlib \n  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n  color_image = lab2rgb(color_image.astype(np.float64))\n  grayscale_input = grayscale_input.squeeze().numpy()\n  if save_path is not None and save_name is not None: \n    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:34.982752Z","iopub.execute_input":"2022-04-07T08:31:34.983112Z","iopub.status.idle":"2022-04-07T08:31:34.998930Z","shell.execute_reply.started":"2022-04-07T08:31:34.983077Z","shell.execute_reply":"2022-04-07T08:31:34.997955Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Make folders and set parameters\nos.makedirs('outputs/color', exist_ok=True)\nos.makedirs('outputs/gray', exist_ok=True)\nos.makedirs('checkpoints', exist_ok=True)\nsave_images = True\nbest_losses = 1e10\nepochs = 200\n\nif use_gpu: \n  criterion = criterion.cuda()\n  model = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:35.000517Z","iopub.execute_input":"2022-04-07T08:31:35.000862Z","iopub.status.idle":"2022-04-07T08:31:37.783068Z","shell.execute_reply.started":"2022-04-07T08:31:35.000821Z","shell.execute_reply":"2022-04-07T08:31:37.782274Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Train model\nfor epoch in range(epochs):\n  # Train for one epoch, then validate\n  train(train_loader, model, criterion, optimizer, epoch)\n  with torch.no_grad():\n    losses = validate(val_loader, model, criterion, save_images, epoch)\n  # Save checkpoint and replace old best model if current model is better\n  if losses < best_losses:\n    best_losses = losses\n    torch.save(model.state_dict(), 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T08:31:37.784528Z","iopub.execute_input":"2022-04-07T08:31:37.784763Z","iopub.status.idle":"2022-04-07T14:42:37.248039Z","shell.execute_reply.started":"2022-04-07T08:31:37.784729Z","shell.execute_reply":"2022-04-07T14:42:37.247238Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Show images \nimport matplotlib.image as mpimg\n\nimage_pairs = [('outputs/color/img-3-epoch-199.jpg', 'outputs/gray/img-3-epoch-0.jpg'),\n               ('outputs/color/img-9-epoch-199.jpg', 'outputs/gray/img-9-epoch-0.jpg')]\n               \nfor c, g in image_pairs:\n  color = mpimg.imread(c)\n  gray  = mpimg.imread(g)\n  f, axarr = plt.subplots(1, 2)\n  f.set_size_inches(15, 15)\n  axarr[0].imshow(gray, cmap='gray')\n  axarr[1].imshow(color)\n  axarr[0].axis('off'), axarr[1].axis('off')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:47:27.691027Z","iopub.execute_input":"2022-04-07T14:47:27.691286Z","iopub.status.idle":"2022-04-07T14:47:28.284713Z","shell.execute_reply.started":"2022-04-07T14:47:27.691257Z","shell.execute_reply":"2022-04-07T14:47:28.281525Z"},"trusted":true},"execution_count":39,"outputs":[]}]}